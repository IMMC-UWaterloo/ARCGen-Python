# -*- coding: utf-8 -*-
"""ArcGen.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q_Te8Jcj0txH5tTHXlQVspyB1mg586sh
"""

import numpy as np
from numpy import inf
import matplotlib.pyplot as plt
from matplotlib.patches import Ellipse
import matplotlib.transforms as transforms
from matplotlib import path
import matplotlib.cm as cm
from scipy import interpolate
from scipy import optimize
import statistics
from shapely import geometry
from shapely.geometry import Point, Polygon, LineString, shape
import math

#%% Function used to evaluate correlation score between signals
def evalCorrScore(signalsX, signalsY):
  #% Correlation score taken from the work of Nusholtz et al. (2009)
  #% Compute cross-correlation matrix of all signals to each other
  corrMatX = np.corrcoef(signalsX, rowvar=False)
  corrMatY = np.corrcoef(signalsY, rowvar=False)
  #% Convert matrices to a single score
  nSignal = len(corrMatX)
  corrScoreX = (1/(nSignal*(nSignal-1)))*(np.sum(np.sum(corrMatX))-nSignal)
  corrScoreY = (1/(nSignal*(nSignal-1)))*(np.sum(np.sum(corrMatY))-nSignal)
  #% Compute a single metric for optimization purposes. Using simple mean
  meanCorrScore = 0.5*(corrScoreX+corrScoreY)
  corrScoreArray = np.column_stack((corrScoreX, corrScoreY))
  return meanCorrScore, corrScoreArray

#%% Function used to compute objective for optimization
def warpingObjective(optimWarp,nCtrlPts,inputSignals,WarpingPenalty,nResamplePoints):
  #% Control points are equally spaced in arc-length.
  #% optimwarp is a column vector with first warped control point in the
  #% first nSignal indices, then 2nd control point in the next nSignal indices

  nSignal = len(inputSignals)
  warpArray = optimWarp.reshape((2*nSignal, nCtrlPts), order='F')
  #% Compute a warping penalty
  penaltyScore = warpingPenalty(warpArray, WarpingPenalty, nResamplePoints, nSignal)
  penaltyScore = np.mean(penaltyScore, axis=0)

  # % Perform warping - non-mex version
  #% IMPORTANT: This is a compiled mex verison of warpArcLength. The mex
  #% function cannot be modified. If warpArcLength is updated later, you will
  #% also need to recompile the mex function
  _, signalsX, signalsY = warpArcLength(warpArray, inputSignals, nResamplePoints)
  #% Compute correlation score
  corrScore, _ = evalCorrScore(signalsX, signalsY)
  #% corrScore is a maximization goal. Turn into a minimization goal
  optScore = 1-corrScore+penaltyScore
  return optScore

#%% Function used to warp arc-length function
def warpArcLength(warpArray, inputSignals, nResamplePoints):

  #% Warp array: each row is warping points for an input signal, each column
  #% is warped point. Control points are interpolated  on [0,1] assuming
  #% equal spacing.
  nSignal = len(inputSignals)

  #% Initialize matrices
  signalsX = np.zeros((nResamplePoints, nSignal))
  signalsY = np.zeros((nResamplePoints, nSignal))
  warpedSignals = {}
  
  for iSignal, keys in enumerate(inputSignals):
    signal = inputSignals[keys]
    lmCtrlPts = np.concatenate([[0], warpArray[iSignal + nSignal, :], [1]])

    #% prepend 0 and append 1 to warp points for this signal to create valid
    #% control points.
    warpedCtrlPts = np.concatenate([[0], warpArray[iSignal,:], [1]])

    #% Construct warping function using SLM. This warps lmAlen to shiftAlen.
    #% Use warping fuction to map computed arc-lengths onto the shifted
    #% system. use built-in pchip function. This is a peicewise monotonic
    #% cubic spline. Signifincantly faster than SLM.
    warpedNormAlen = interpolate.pchip(lmCtrlPts,warpedCtrlPts)(signal[:,3])

    #% Now uniformly resample normalzied arc-length
    resamNormwarpedAlen = np.linspace(0, 1, num = nResamplePoints)
    resampX = interpolate.interp1d(warpedNormAlen, signal[:, 0], kind='linear', fill_value="extrapolate")(resamNormwarpedAlen)
    resampY = interpolate.interp1d(warpedNormAlen, signal[:, 1], kind='linear', fill_value="extrapolate")(resamNormwarpedAlen)

    #% Assign to array for correlation calc
    signalsX[:, iSignal] = resampX
    signalsY[:, iSignal] = resampY

    #% Assemble a cell array containing arrays of resampled signals. Similar
        #% to 'normalizedSignal' in 'inputSignals' structure
    warpedSignals[keys] = np.column_stack((resamNormwarpedAlen, resampX, resampY))
  return warpedSignals, signalsX, signalsY

#%% Penalty function to prevent plateaus and extreme divergence in warping functions
def warpingPenalty(warpArray, WarpingPenalty, nResamplePoints, nSignal):
#% Compute an array of penalty scores based on MSE between linear, unwarped
#% arc-length and warped arc-length. Aim is to help prevent plateauing.
  nSignals = nSignal

  penaltyScores = np.zeros((nSignals));
  unwarpedAlen = np.linspace(0, 1, num = nResamplePoints);
  for iSignal in range(nSignals):
    interpX = np.concatenate([[0],warpArray[iSignal+nSignals,:],[1]], axis=None, dtype='float')
    interpY = np.concatenate([[0],warpArray[iSignal,:],[1]], axis=None, dtype='float')
    interpResults = interpolate.pchip(interpX, interpY, axis=0)(unwarpedAlen)
    penaltyScores[iSignal] = np.sum(((unwarpedAlen - interpResults)**2))
  penaltyScores = WarpingPenalty * penaltyScores
  return penaltyScores

#%% helper function to perform linear interpolation to an isovalue of 1 only
def interpVal(x1, y1, x2, y2):
  val = x1+(x2-x1)*(1-y1)/(y2-y1)
  return val

def polyxpoly(x1, y1, x2, y2):

  line1 = LineString(np.column_stack((x1, y1)))
  line2 = LineString(np.column_stack((x2, y2)))

  intersPts = line1.intersection(line2)
  xInterc = [ ((dumm.xy[0][0])) for dumm in intersPts ]
  yInterc = [ ((dumm.xy[1][0])) for dumm in intersPts ]
  oneIndex = np.array([])
  twoIndex = np.array([])

  line1Pts = list(line1.coords)
  line2Pts = list(line2.coords)

  for index1, (i,j) in enumerate(zip(line1Pts, line1Pts[1:])):
    for indInterc in range(len(intersPts)):
      if LineString((i,j)).distance(intersPts[indInterc]) < 1E-9:
        oneIndex = np.append(oneIndex, index1)
  
  for index2, (i,j) in enumerate(zip(line2Pts, line2Pts[1:])):
    for indInterc in range(len(intersPts)):
      if LineString((i,j)).distance(intersPts[indInterc]) < 1E-9:
        twoIndex = np.append(twoIndex, index2)
  return xInterc, yInterc, np.column_stack((oneIndex, twoIndex)).astype(int)

def inpolygon(xq, yq, xv, yv):
  points = list(zip(xq,yq))
  polyPoints = list(zip(xv,yv))
  p = []
  for i in range(len(points)):
    p.append(Point(points[i]))
  line = LineString(polyPoints)

  polygon = Polygon(line)
  result = []
  for i in range(len(points)):
    if polygon.contains(p[i]) or p[i].intersects(polygon.buffer(1e-12)):
      result.append(1)
    else:
      result.append(0)
  return result

def find(array, string = None):
  array = np.array(array)
  n = array.ndim
  if n ==0:
    return []
  elif n == 1:
    result = np.argwhere(array)
  elif n == 2:
    array1d = array.ravel(order='F')
    result = np.argwhere(array1d)
  else:
    raise ValueError("Array dimensions not supported")

  if string == 'first' and result.size != 0:
    return np.sort(result, axis = None)[0]
  elif string == 'last' and result.size != 0:
    return np.sort(result, axis = None)[-1]
  else:
    return np.array(result).astype(int)

def ispolycw(xv, yv):
  polyPoints = list(zip(xv,yv))
  line = LineString(polyPoints)

  polygon = Polygon(line)
  if polygon.exterior.is_ccw:
    result = False
  else:
    result= True
  return result

#%% Setup Name-Value Argument parser
nResamplePoints = 200
Diagnostics = 'on'
CorridorScaleFact = 1
NormalizeSignals = 'off'
EllipseKFact = 1
CorridorRes = 200
MinCorridorWidth = 0
nWarpCtrlPts = 0
WarpingPenalty = 1e-2
KeepUnmatched = True
CaseSensitive = False


#%%Declarations
inputSignals ={}
maxAlen = {}
meanDev = {}
medianDevs = {}
normalizedSignal = {}
xMax = {}
xMin = {}
yMax = {}
yMin = {}
xNormMax = {}
yNormMax = {}
warpControlPoints = {}

dataframe = np.genfromtxt('/content/drive/MyDrive/Colab Notebooks/data.csv', delimiter=',', encoding=None)
numberRows, numberCols = dataframe.shape

if numberCols % 2 == 0:
  numberSignals = int(numberCols/2)
else:
  raise ValueError("The number of columns in the csv file is not even")    

for i in range(len(np.hsplit(dataframe,numberSignals))):
    inputSignals['Signal '+str(i+1)] = np.hsplit(dataframe,numberSignals)[i]


for iSignal in inputSignals.keys():
  plt.plot(inputSignals[iSignal][:,0], inputSignals[iSignal][:,1], label = iSignal)

plt.xlabel('x - axis')
# Set the y axis label of the current axis.
plt.ylabel('y - axis')
# Set a title of the current axes.
plt.title('Input Signals')
# show a legend on the plot
plt.legend()
# Display a figure.
plt.show()
inputSignals['Signal 1'][0][0]=0

#%% Compute arclength based on input signal datapoints
#% Do not perform normalization
if NormalizeSignals == 'off':
    for iSignal in inputSignals.keys():
        temp = inputSignals[iSignal] # Temporary for conveinenc

        # % Compute arc - length between each data point
        segments = np.sqrt((temp[0:-1,0] - temp[1:,0])**2 + (temp[0:-1, 1] - temp[1:, 1])**2)
        segments = np.append([0], segments)

        #% Append cumulative arc length to data array
        alen = np.cumsum(segments)

        #% Compute normalized arc - length
        maxAlen[iSignal] = np.max(alen)
        inputSignals[iSignal] = np.column_stack((inputSignals[iSignal], alen))
        inputSignals[iSignal] = np.column_stack((inputSignals[iSignal], alen/maxAlen[iSignal]))

        #% Determine max[x, y] data
        xMax[iSignal], yMax[iSignal] = temp.max(axis=0)

        #% Remove spurious duplicates
        _,idx=np.unique(inputSignals[iSignal], axis=0,return_index=True)
        inputSignals[iSignal][np.sort(idx)]

#% Perform magnitude normalization based on bounding box
if NormalizeSignals =='on':
    #% Determine bounding box of individual signals
    for iSignal in inputSignals.keys():
        temp = inputSignals[iSignal]  # Temporary for conveinenc

    #% Determine min[x, y] and max[x, y]data
        xMin[iSignal], yMin[iSignal]  = temp.min(axis=0)
        xMax[iSignal], yMax[iSignal] = temp.max(axis=0)

    xBound = [sum(xMin.values())/len(xMin.values()), sum(xMax.values())/len(xMax.values())]
    yBound = [sum(yMin.values())/len(yMin.values()), sum(yMax.values())/len(yMax.values())]

    #% Normalize the axis of each signal, then do arc-length calcs
    for iSignal in inputSignals.keys():
        temp = inputSignals[iSignal] # Temporary for conveinenc

        #% Normalize from bounding box to [-1,1]
        temp[:,0] = temp[:,0] / (xBound[1]-xBound[0])
        temp[:,1] = temp[:,1] / (yBound[1]-yBound[0])

        # % Compute arc - length between each data point
        segments = np.sqrt((temp[0:-1, 0] - temp[1:, 0]) ** 2 + (temp[0:-1, 1] - temp[1:, 1])** 2)
        segments = np.append([0], segments)

        # % Append cumulative arc length to data array
        alen = np.cumsum(segments)

        # % Compute normalized arc - length
        maxAlen[iSignal] = np.max(alen)
        inputSignals[iSignal] = np.column_stack((inputSignals[iSignal], alen))
        inputSignals[iSignal] = np.column_stack((inputSignals[iSignal], alen / maxAlen[iSignal]))

        # % Determine max[x, y] data
        xNormMax[iSignal], yNormMax[iSignal] = temp.max(axis=0)

        # % Remove spurious duplicates
        inputSignals[iSignal] = np.unique(inputSignals[iSignal], axis=0)

#% Compute mean and median arc - length deviation
meanAlen = sum(maxAlen.values())/len(maxAlen.values())
medianAlen = statistics.median(maxAlen.values())

for iSignal in inputSignals.keys():
    meanDev[iSignal] = maxAlen[iSignal] - meanAlen
    medianDevs[iSignal] = maxAlen[iSignal] - medianAlen

    normAlen = np.linspace(0, inputSignals[iSignal][-1, 3], num = nResamplePoints)

    resampX = interpolate.interp1d(inputSignals[iSignal][:, 3], inputSignals[iSignal][:, 0])(normAlen)
    resampY = interpolate.interp1d(inputSignals[iSignal][:, 3], inputSignals[iSignal][:, 1])(normAlen)

    #% Resulting array is normalized arc - length, resampled x, resam.y
    normalizedSignal[iSignal] = np.column_stack((normAlen, resampX, resampY))
    plt.plot(normalizedSignal[iSignal][:,1], normalizedSignal[iSignal][:,2], label = iSignal)
plt.xlabel('x - axis')
# Set the y axis label of the current axis.
plt.ylabel('y - axis')
# Set a title of the current axes.
plt.title('Normalized Signals')
# show a legend on the plot
plt.legend()
# Display a figure.
plt.show()

#% % For each resampled point, determine average and standard deviation across signals
#% Initialize arrays
charAvg = np.zeros((nResamplePoints, 2))
stdevData = np.zeros((nResamplePoints, 2))

for iPoints in range(nResamplePoints):
    temp = np.empty([0, 2])

    for iSignal in inputSignals.keys():
        temp = np.vstack((temp, normalizedSignal[iSignal][iPoints,1:3]))
    charAvg[iPoints,:]= temp.mean(axis=0)
    stdevData[iPoints,:] = temp.std(axis=0)

debugOutput_charAvg = charAvg
debugOutput_stdevData = stdevData

#%% Align normalized arc-length signals based on minimized correlation.
#% Enabled by option 'nWarpCtrlPts'. If 0, skip alignment.
if nWarpCtrlPts > 0:
  #% Assemble signal matrices prior to correlation
  signalX = np.zeros((nResamplePoints, len(inputSignals)))
  signalY = np.zeros((nResamplePoints, len(inputSignals)))
  for i in range(len(inputSignals)):
    signalX[:,i] = normalizedSignal[list(inputSignals)[i]][:, 1]
    signalY[:,i] = normalizedSignal[list(inputSignals)[i]][:, 2]
  meanCorrScore, corrArray = evalCorrScore(signalX,signalY)
  #% Assign pre-optimized correlation scores to debug structure
  preWarpCorrArray = corrArray;
  preWarpMeanCorrScore = meanCorrScore;

  #% Optimize warp points for arbitrary n warping points. Build bounds,
  #% constraints, and x0s
  nWarp = nWarpCtrlPts
  nSignal = numberSignals

  if nWarp == 1:   #% nWarp == 1 is a special case as inequalites aren't needed
    x0 = 0.5 * np.ones((nSignal*2))
    lb = 0.15 * np.ones((nSignal*2))
    ub = 0.85 * np.ones((nSignal*2))
    A = []
    b = []
  elif nWarp >= 15:
    print('Specifying more than 10 interior warping points is not supported')
  else:
    x0 = np.zeros((nWarp * (nSignal * 2)))

    for i in range(nWarp):
      x0[np.array([((i) * nSignal)+ np.arange(0, nSignal)+(i * (nSignal))])] = ((i+1)/(nWarp+1)) * np.ones((nSignal))
      x0[np.array([((i) * nSignal) + np.arange(0, nSignal) + ((i+1) * nSignal)])] = ((i+1)/(nWarp+1)) * np.ones((nSignal))

    lb = 0.05 * np.ones((nWarp*(nSignal*2)))
    ub = 0.95 * np.ones((nWarp*(nSignal*2)))
    A = np.zeros([((nWarp-1) * (nSignal * 2)), (nWarp * (nSignal * 2))])
    b = -0.05 * np.ones(((nWarp) * (nSignal * 2)))  # % Force some separation between warped points
    
    for iSignal in range(nSignal * 2):
      for iWarp in range(nWarp-1):
        A[iSignal + (iWarp) * (nSignal * 2), iSignal + ((iWarp) * (nSignal * 2))] = 1
        A[iSignal + (iWarp) * (nSignal * 2), iSignal + ((iWarp+1) * (nSignal * 2))] = -1
  #% Execute optimization and compute warped signals
  #optWarpArray = optimize.minimize(warpingObjective(x, nWarp,inputSignals, WarpingPenalty,nResamplePoints),x0, A, b, [], [], lb, ub, [], optOptions);
  bounds = list(zip(lb, ub))
  #linear_constraint = LinearConstraint(A,-np.inf*b , 0.9*b)
  #optWarpArray = optimize.minimize(warpingObjective, x0, args= (nWarp,inputSignals, WarpingPenalty,nResamplePoints), method='trust-constr', jac=None, hess=None, constraints=([linear_constraint []]), options={'maxiter': 3000}, bounds=bounds)
  #optWarpArrayX = optimize.minimize(warpingObjective, x0, args=(nWarp,inputSignals, WarpingPenalty,nResamplePoints), method='trust-constr', hess=None, hessp=None, bounds=bounds, constraints=(linear_constraint),  options={ 'gtol': 1e-08, 'disp': False})
  optWarpArrayX = optimize.minimize(warpingObjective, x0, args=(nWarp,inputSignals, WarpingPenalty,nResamplePoints), method = 'SLSQP', bounds=bounds, options={ 'disp': False})
  #, constraints=linear_constraint, bounds=bounds, options={'gtol': 1e-6, maxiter=3000})
  #optWarpArray = optimize.linprog(warpingObjective, A_ub=A, b_ub=b, A_eq=None, b_eq=None, bounds=(lb,ub), method='interior-point', callback=None, 
  #                                 options={'maxiter': 1000, 'disp': False, 'presolve': True, 'tol': 1e-08, 'autoscale': False, 'rr': True, 'alpha0': 0.99995, 
      #                                          'beta': 0.1, 'sparse': False, 'lstsq': False, 'sym_pos': True, 'cholesky': True, 'pc': True, 'ip': False, 'permc_spec': 'MMD_AT_PLUS_A'}, 
      #                                 x0=x0)
  optWarpArray =optWarpArrayX.x
  optWarpArray = optWarpArray.reshape((nSignal*2,nWarp), order='F')

  warpedSignals, signalX, signalY = warpArcLength(optWarpArray,inputSignals,nResamplePoints)


  #% Compute correlation score
  meanCorrScore, corrArray = evalCorrScore(signalX, signalY)
  #% Assign warped correlation scores to debug structure
  warpedCorrArray = corrArray
  warpedMeanCorrScore = meanCorrScore

      
  #% Replace 'normalizedSignal' in 'responseSignal' and compute average and
  #% standard deviation.
  for iSignal, keys in enumerate(inputSignals):
    normalizedSignal[keys] = warpedSignals[keys]
    tempRow1 = np.concatenate([[0],optWarpArray[iSignal+nSignal,:],[1]])
    tempRow2 = np.concatenate([[0],optWarpArray[iSignal,:],[1]])
    warpControlPoints[keys] = np.concatenate([[tempRow1], [tempRow2]])

  for iPoints in range(nResamplePoints):
    temp = np.empty((nSignal, 2)) #% probably cleaner way to do this.
    for iSignal, keys in enumerate(inputSignals):
      #% collect specific point from each signal
      temp[iSignal,:] = normalizedSignal[keys][iPoints,1:3]
    charAvg[iPoints,:] = np.mean(temp, axis = 0)
    stdevData[iPoints,:] = np.std(temp, axis = 0,  ddof=1)

#%% Clamp minimum corridor width. Disabled if 'MinCorridorWidth' == 0
#% Include influence of corridor scaling factor 'EllipseKFact'
if MinCorridorWidth > 0:
  #% Replace any stDevData below maximum st.dev. * 'MinCorridorWidth'
  stdevData[:,0] = np.where(stdevData[:,0] < (MinCorridorWidth * np.max(stdevData[:,0]) * EllipseKFact), (MinCorridorWidth * np.max(stdevData[:,0]) * EllipseKFact), stdevData[:,0])
  stdevData[:,1] = np.where(stdevData[:,1] < (MinCorridorWidth * np.max(stdevData[:,1]) * EllipseKFact), (MinCorridorWidth * np.max(stdevData[:,1]) * EllipseKFact), stdevData[:,1])

#%% Diagnostic: Plot normalized signals and St. Devs. 
if Diagnostics == 'on' or Diagnostics == 'detailed':
    
  #% Plot normalized x,y 
  color = list(np.random.choice(range(256), size=3))
  fig, ax = plt.subplots(2, 2, figsize = (8,6), dpi = 300)
  fig.suptitle('Arc-length Discretized Normalized Signals')
  ax[0][0].title.set_text('Arc-length Discretized Normalized Signals')
  ax[0][1].title.set_text('Warping Functions')
  ax[1][0].title.set_text('Average and St.Dev. of X-Data')
  ax[1][1].title.set_text('Average and St.Dev. of Y-Data')


  for iSignal, keys in enumerate(inputSignals):
    ax[0][0].plot(normalizedSignal[keys][:,1], normalizedSignal[keys][:,2], label= keys)
    
    if nWarpCtrlPts > 0:
      interpX = np.concatenate([[0],optWarpArray[iSignal+nSignal,:],[1]], axis=None, dtype='float')
      interpY = np.concatenate([[0],optWarpArray[iSignal,:],[1]], axis=None, dtype='float')
      interpResults = interpolate.pchip(interpX, interpY, axis=0)(inputSignals[keys][:,3])
      ax[0][1].plot(inputSignals[keys][:,3],interpResults, label = keys )
    else: 
        ax[0][1].title.set_text('No Warping Performed')

    ax[1][0].plot(normalizedSignal[list(normalizedSignal.keys())[0]][:,0], normalizedSignal[keys][:,1], label= keys)
    ax[1][0].errorbar(normalizedSignal[list(normalizedSignal.keys())[0]][:,0], charAvg[:,0], yerr=stdevData[:,0])
    ax[1][1].plot(normalizedSignal[list(normalizedSignal.keys())[0]][:,0], normalizedSignal[keys][:,2], label= keys)
    ax[1][1].errorbar(normalizedSignal[list(normalizedSignal.keys())[0]][:,0], charAvg[:,1], yerr=stdevData[:,1])
      # if NormalizeSignals == 'off' or NormalizeSignals == 'on':
      #   continue
      # else
      #   plot(inputSignals(iSignal).data(inputSignals(iSignal).alignInd,1),...
      #   inputSignals(iSignal).data(inputSignals(iSignal).alignInd,2),'kx','LineWidth',2.0)

    
  #ax[0][1].plot(interpX, interpY )
  ax[0][1].plot([0,1],[0,1])
  ax[0][0].set(xlabel='x-data', ylabel='y-data')
  ax[0][1].set(xlabel='Unwarped Normalized Arc-length', ylabel='Warped Normalized Arc-length')
  ax[1][0].set(xlabel='Normalized Arc-length', ylabel='x-data')
  ax[1][1].set(xlabel='Normalized Arc-length', ylabel='y-data')


#     xlabel('Normalized Arc-length')
#     ylabel('y-data')
#     title('Average and St.Dev. of Y-Data')
  fig.tight_layout()
  fig.subplots_adjust(top=0.91)
  fig.show()
  ax[0][0].legend(loc='lower right')

if Diagnostics == 'on':
  fig, ax = plt.subplots(figsize = (6,4), dpi = 300)
  ellipse_xy = list(zip(charAvg[:,0], charAvg[:,1]))
  for iPoint in range(nResamplePoints):
    ellipse = Ellipse(ellipse_xy[iPoint], stdevData[iPoint,0] * EllipseKFact*2, stdevData[iPoint,1] * EllipseKFact*2, angle = 0, edgecolor='grey', lw=0.7, facecolor='none')
    ax.add_artist(ellipse)
  ax.plot(charAvg[:,0], charAvg[:,1], label= 'Char Avg')
  for iSignal in inputSignals.keys():
    ax.plot(inputSignals[iSignal][:,0], inputSignals[iSignal][:,1], label = iSignal)


  plt.xlabel('x - axis')
# Set the y axis label of the current axis.
  plt.ylabel('y - axis')
# Set a title of the current axes.
  fig.suptitle('Ellipses and Corridor Extraction ')
# show a legend on the plot
  fig.legend()
# Display a figure.
  fig.show()
  # % Plot ellipses

#%% Begin marching squares algorithm
#% Create grids based on upper and lower of characteristic average plus 120%
#% of maximum standard deviation
scaleFact = 1.2 * EllipseKFact
xx,yy = np.meshgrid(np.linspace((np.min(charAvg[:,0]) - scaleFact*np.max(stdevData[:,0])), (np.max(charAvg[:,0]) + scaleFact * np.max(stdevData[:,0])), num = CorridorRes), np.linspace((np.min(charAvg[:,1]) - scaleFact*np.max(stdevData[:,1])), (np.max(charAvg[:,1]) + scaleFact*np.max(stdevData[:,1])), num = CorridorRes), indexing='xy')
zz = np.zeros(np.shape(xx))   #% initalize grid of ellipse values
# #% For each grid point, find the max of each standard deviation ellipse
kFact = EllipseKFact #% faster if no struct call in inner loop. 
nRes = CorridorRes   #% again, for speed
for iPt in range(nRes):
  for jPt in range(nRes):
    zz[iPt,jPt] = np.max(
        (((xx[iPt,jPt] - charAvg[:,0])**2 / (stdevData[:,0]*kFact)**2 + (yy[iPt,jPt] - charAvg[:,1])**2 / (stdevData[:,1]*kFact)**2)**-1)
                        )

# % The following segments is the marching squares algorith. The goal of this
# % algorithm is to find the zz=1 isoline, as this represents the outer
# % boundary of all elllipses. 
# %
# % Described in brief, this algorithm goes through each point, looking at
# % its and its neighbours values. There are only 16 configurations of these
# % squares or cells. Based on the configuration, add the appropriate line
# % segments. This method uses linear interpolation to increase accuracy. 
# % Initalize line segments for speed. This line may cause issues, as it
# % assumes maximum size. Bump up 10 if it does. 
lineSegments = np.zeros((10*max(nResamplePoints,CorridorRes), 4))
iSeg = -1
for iPt in range(CorridorRes-1):  #% Rows (y-axis)
  for jPt in range(CorridorRes-1):   #% Columns (x-axis)
        # % Cell value definition
        # %  1 -- 2 
        # %  |    |
        # %  |    |
        # %  8 -- 4
        # %
        # % REMEMBER!!!! 
        # % array(i,j) = array(rows, columns,) = array(y,x)
        
        # % By carefully defining cell values and definitions, we can use
        # % binary to simplify logic though a integer based switch case        % 
    cellValue = int((1 * (zz[iPt,jPt]>1)) + (2 * (zz[iPt+1,jPt]>1)) + (4 * (zz[iPt+1,jPt+1]>1)) + (8 * (zz[iPt,jPt+1]>1)) + 1)
    
    if cellValue == 1:
      #% No Vertices
      pass
    elif  cellValue == 2:
      #% South-West
      iSeg = iSeg+1
      lineSegments[iSeg,:] = [interpVal(xx[iPt,jPt],zz[iPt,jPt],xx[iPt,jPt+1],zz[iPt,jPt+1]),yy[iPt,jPt],
      xx[iPt,jPt], interpVal(yy[iPt,jPt],zz[iPt,jPt],yy[iPt+1,jPt],zz[iPt+1,jPt])]

    elif  cellValue == 3:
      #% West-North
      iSeg = iSeg+1
      lineSegments[iSeg,:] = [xx[iPt+1,jPt],interpVal(yy[iPt,jPt],zz[iPt,jPt], yy[iPt+1,jPt],zz[iPt+1,jPt]),
          interpVal(xx[iPt+1,jPt],zz[iPt+1,jPt], xx[iPt+1,jPt+1],zz[iPt+1,jPt+1]),yy[iPt+1,jPt]]
    
    elif  cellValue == 4:
      #% North-South
      iSeg = iSeg+1;
      lineSegments[iSeg,:] = [interpVal(xx[iPt,jPt],zz[iPt,jPt],xx[iPt,jPt+1],zz[iPt,jPt+1]),yy[iPt,jPt],
          interpVal(xx[iPt+1,jPt],zz[iPt+1,jPt],xx[iPt+1,jPt+1], zz[iPt+1,jPt+1]),yy[iPt+1,jPt]]
          
    elif  cellValue == 5:
      #% North-East
      iSeg = iSeg+1;
      lineSegments[iSeg,:] = [interpVal(xx[iPt+1,jPt],zz[iPt+1,jPt],xx[iPt+1,jPt+1],zz[iPt+1,jPt+1]),yy[iPt+1,jPt+1],
          xx[iPt+1,jPt+1],interpVal(yy[iPt+1,jPt+1],zz[iPt+1,jPt+1], yy[iPt,jPt+1], zz[iPt,jPt+1])]
          
    elif  cellValue == 6:  #% Ambiguous 
      centerVal = np.mean(list([zz[iPt,jPt], zz[iPt+1,jPt], zz[iPt+1,jPt+1], zz[iPt, jPt+1]]))
      if centerVal >= 1:
        #% West-North
        iSeg = iSeg+1
        lineSegments[iSeg,:] = [xx[iPt+1,jPt],interpVal(yy[iPt,jPt],zz[iPt,jPt], yy[iPt+1,jPt],zz[iPt+1,jPt]),
            interpVal(xx[iPt+1,jPt],zz[iPt+1,jPt], xx[iPt+1,jPt+1],zz[iPt+1,jPt+1]),yy[iPt+1,jPt]]
              
        #% South-East
        iSeg = iSeg+1
        lineSegments[iSeg,:] = [interpVal(xx[iPt,jPt+1],zz[iPt,jPt+1],xx[iPt,jPt],zz[iPt,jPt]),yy[iPt,jPt+1],
            xx[iPt,jPt+1],interpVal(yy[iPt,jPt+1],zz[iPt,jPt+1],yy[iPt+1,jPt+1],zz[iPt+1,jPt+1])]
      else:
        #% South-West
        iSeg = iSeg+1
        lineSegments[iSeg,:] = [interpVal(xx[iPt,jPt],zz[iPt,jPt],xx[iPt,jPt+1],zz[iPt,jPt+1]),yy[iPt,jPt],
            xx[iPt,jPt], interpVal(yy[iPt,jPt],zz[iPt,jPt],yy[iPt+1,jPt],zz[iPt+1,jPt])]
              
        #% North-East
        iSeg = iSeg+1
        lineSegments[iSeg,:] = [interpVal(xx[iPt+1,jPt],zz[iPt+1,jPt],xx[iPt+1,jPt+1],zz[iPt+1,jPt+1]),yy[iPt+1,jPt+1],
            xx[iPt+1,jPt+1],interpVal(yy[iPt+1,jPt+1],zz[iPt+1,jPt+1], yy[iPt,jPt+1], zz[iPt,jPt+1])]

    elif  cellValue == 7:
      #% West-East
      iSeg = iSeg+1
      lineSegments[iSeg,:] = [xx[iPt,jPt],interpVal(yy[iPt,jPt],zz[iPt,jPt],yy[iPt+1,jPt],zz[iPt+1,jPt]),
          xx[iPt,jPt+1],interpVal(yy[iPt,jPt+1],zz[iPt,jPt+1],yy[iPt+1,jPt+1],zz[iPt+1,jPt+1])]
    
    elif  cellValue == 8:
      #% South - East
      iSeg = iSeg+1;
      lineSegments[iSeg,:] = [interpVal(xx[iPt,jPt+1],zz[iPt,jPt+1],xx[iPt,jPt],zz[iPt,jPt]),yy[iPt,jPt+1],
          xx[iPt,jPt+1],interpVal(yy[iPt,jPt+1],zz[iPt,jPt+1],yy[iPt+1,jPt+1],zz[iPt+1,jPt+1])]
                                    
    elif  cellValue == 9:
      #% South - East
      iSeg = iSeg+1
      lineSegments[iSeg,:] = [interpVal(xx[iPt,jPt+1],zz[iPt,jPt+1],xx[iPt,jPt],zz[iPt,jPt]),yy[iPt,jPt+1],
          xx[iPt,jPt+1],interpVal(yy[iPt,jPt+1],zz[iPt,jPt+1],yy[iPt+1,jPt+1],zz[iPt+1,jPt+1])]

    elif  cellValue == 10:
      #% West-East
      iSeg = iSeg+1
      lineSegments[iSeg,:] = [xx[iPt,jPt],interpVal(yy[iPt,jPt],zz[iPt,jPt],yy[iPt+1,jPt],
          zz[iPt+1,jPt]), xx[iPt,jPt+1],interpVal(yy[iPt,jPt+1],zz[iPt,jPt+1],yy[iPt+1,jPt+1],zz[iPt+1,jPt+1])]

    elif  cellValue == 11: #% Ambiguous
      centerVal = np.mean(list([zz[iPt,jPt], zz[iPt+1,jPt], zz[iPt+1,jPt+1], zz[iPt, jPt+1]]))
      if centerVal >= 1:
        #% South-West
        iSeg = iSeg+1
        lineSegments[iSeg,:] = [interpVal(xx[iPt,jPt],zz[iPt,jPt],xx[iPt,jPt+1],zz[iPt,jPt+1]),yy[iPt,jPt],
            xx[iPt,jPt], interpVal(yy[iPt,jPt],zz[iPt,jPt],yy[iPt+1,jPt],zz[iPt+1,jPt])]

        #% North-East
        iSeg = iSeg+1
        lineSegments[iSeg,:] = [interpVal(xx[iPt+1,jPt],zz[iPt+1,jPt],xx[iPt+1,jPt+1],zz[iPt+1,jPt+1]),yy[iPt+1,jPt+1],
            xx[iPt+1,jPt+1],interpVal(yy[iPt+1,jPt+1],zz[iPt+1,jPt+1], yy[iPt,jPt+1], zz[iPt,jPt+1])]
      else:
        #% West-North
        iSeg = iSeg+1
        lineSegments[iSeg,:] = [xx[iPt+1,jPt],interpVal(yy[iPt,jPt],zz[iPt,jPt], yy[iPt+1,jPt],zz[iPt+1,jPt]),
              interpVal(xx[iPt+1,jPt],zz[iPt+1,jPt], xx[iPt+1,jPt+1],zz[iPt+1,jPt+1]),yy[iPt+1,jPt]]
        #% South-East
        iSeg = iSeg+1
        lineSegments[iSeg,:] = [interpVal(xx[iPt,jPt+1],zz[iPt,jPt+1],xx[iPt,jPt],zz[iPt,jPt]),yy[iPt,jPt+1],
            xx[iPt,jPt+1],interpVal(yy[iPt,jPt+1],zz[iPt,jPt+1],yy[iPt+1,jPt+1],zz[iPt+1,jPt+1])]

    elif  cellValue == 12:
      #% North-East
      iSeg = iSeg+1
      lineSegments[iSeg,:] = [interpVal(xx[iPt+1,jPt],zz[iPt+1,jPt],xx[iPt+1,jPt+1],zz[iPt+1,jPt+1]),yy[iPt+1,jPt+1],
          xx[iPt+1,jPt+1],interpVal(yy[iPt+1,jPt+1],zz[iPt+1,jPt+1], yy[iPt,jPt+1], zz[iPt,jPt+1])]
    
    elif  cellValue == 13:
      #% North-South
      iSeg = iSeg+1
      lineSegments[iSeg,:] = [interpVal(xx[iPt,jPt],zz[iPt,jPt],xx[iPt,jPt+1],zz[iPt,jPt+1]),yy[iPt,jPt],
          interpVal(xx[iPt+1,jPt],zz[iPt+1,jPt],xx[iPt+1,jPt+1], zz[iPt+1,jPt+1]),yy[iPt+1,jPt]]
    elif  cellValue == 14:
      #% West-North
      iSeg = iSeg+1
      lineSegments[iSeg,:] = [xx[iPt+1,jPt],interpVal(yy[iPt,jPt],zz[iPt,jPt], yy[iPt+1,jPt],zz[iPt+1,jPt]),
          interpVal(xx[iPt+1,jPt],zz[iPt+1,jPt], xx[iPt+1,jPt+1],zz[iPt+1,jPt+1]),yy[iPt+1,jPt]]

    elif  cellValue == 15:
      #% South-West
      iSeg = iSeg+1
      lineSegments[iSeg,:] = [interpVal(xx[iPt,jPt],zz[iPt,jPt],xx[iPt,jPt+1],zz[iPt,jPt+1]),yy[iPt,jPt],
          xx[iPt,jPt], interpVal(yy[iPt,jPt],zz[iPt,jPt],yy[iPt+1,jPt],zz[iPt+1,jPt])]

    elif  cellValue == 16:
      pass
      #% No vertices

lineSegments = lineSegments[0:iSeg,:]

#% Extract list of unique vertices from line segmens
vertices = np.vstack((lineSegments[:,0:2],lineSegments[:,2:4]))
vertices = np.unique(vertices, axis= 0)
index = np.empty(len(vertices), dtype=bool)

#% Create a vertex connectivity table. The 1e-12 value is here because
#% floats don't round well and == will not work. 
vertConn = np.zeros((len(lineSegments),2)).astype(int)
for i in range(len(vertConn)):
  index = np.all(np.abs(lineSegments[:,0:2] - vertices[i,:]) < 1e-12, axis = 1)
  vertConn[index,0] = i
  index = np.all(np.abs(lineSegments[:,2:4] - vertices[i,:]) < 1e-12, axis = 1)
  vertConn[index,1] = i

#%% Start line segments sorting and envelope extraction
nEnvelopes = 0
allEnvelopes = np.empty((len(lineSegments),2)).astype(int)
allEnvelopes[0,0] = 0   #% First entry is always vertex 0

temp = np.empty([len(vertConn), 2])
foundShiftedInd = np.empty([len(vertConn),2])
#for i in range(len(vertConn)-1):
for i in range(3):
  j = i + 1 #% helper index

  #% save vertex to find 
  vertToFind = vertConn[i,1].astype(int)
  print('vertToFind', vertToFind, 'i', i)
  print('vertConn from J', vertConn[j:,:].astype(int))

  #% Find connecting node
  vertConnToFind_ = np.any((vertConn[j:,:].astype(int) == vertToFind.astype(int)), axis = 1)
  foundShiftedInd = find(vertConnToFind_.astype(int), 'first')
  print ('foundShiftedInd', foundShiftedInd)

  #% If we have found an index 
  if foundShiftedInd.size > 0:
    foundInd = foundShiftedInd + i

    print(foundInd)
    #% swap found vert conn row with j row
    temp = vertConn[j,:]

    #% Now, decide whether to flip found row. We want vertex 2 of 
    #% previous line to be node 1 of the new line. 
    if vertConn[foundInd,0] == vertToFind:
      vertConn[j,:] = vertConn[foundInd, [0,1]]
      # print('vertConn[foundInd,0] == vertToFind')
    else:
      vertConn[j,:] = vertConn[foundInd, [1,0]]
      
      #% Logic to prevent overwriting, if found row is next row. 
    if foundInd != j:
      vertConn[foundInd,:] = temp
    print('foundInd != j')


  #% If we did not find an index, we either may have an open envelope or
  #% envelope may be convex and loops back on itself. 
  else:
    #% Check to see if we can find the first vertex in envelope
    #% appearing again (check for closure)
    vertConn_ = allEnvelopes[nEnvelopes,0].astype(int)
    vertToFind = vertConn[vertConn_,0] ####### Confirm column
    foundShiftedInd = find(np.any(vertConn[j:,:] == vertToFind, axis = 1), 'first')
    
    #% If we do not find an index, it means this envelope is complete and manifold
    if foundShiftedInd == []:
      #% Assign indices to finish current envelope, initialize next
      allEnvelopes[nEnvelopes,1] = i
      nEnvelopes = nEnvelopes + 1
      allEnvelopes[nEnvelopes, 0] = j
    
    else:
      #% This error should only occur if envelopes extend beyond
      #% sampling grid, which they should not. 
      raise Exception('Literal Edge Case')

allEnvelopes[nEnvelopes,1] = j

#% Find largest envelope
envInds = np.argmax((allEnvelopes[:,1]-allEnvelopes[:,0])).astype(int)

#% Convert indices in evelopes to array of (x,y)
envInds = allEnvelopes[envInds, :]
index = np.arange(envInds[0],envInds[1], 1)
envelope = vertices[vertConn[index,0].astype(int),:]

#%% Divide the envelope into corridors. 
#% To break the largest envelop into inner and outer corridors, we need to
#% account for several edge cases. First, we test to see if there are any
#% intercepts of the characteristic average and the largest envelope. 
closedEnvelope = np.vstack((envelope, envelope[0,:]))

_, _, indexIntercept = polyxpoly(closedEnvelope[:,0],closedEnvelope[:,1],charAvg[:,0],charAvg[:,1])

#% If we find two intercepts, then we have no problem
if len(indexIntercept) >=2:
  iIntStart = indexIntercept[0,0]
  iIntEnd = indexIntercept[-1,0]

#% If we find only one intercept, we need to determine if the intercept is a
#% the start or end of the envelope. Then we need to extend the opposite
#% side of the characteristic average to intercept the envelope. 
elif len(indexIntercept) == 1:
  #% Compute extension 
  aLenInterval = 1/nResamplePoints
  indexLength = round(0.2*len(charAvg))
    
  aLenExtension = np.abs(aLenInterval/(charAvg[0,:]-charAvg[1,:])) * 1.1 * np.max(stdevData)
  aLenExtension[aLenExtension == inf] = 0
  aLenExtension = max(aLenExtension)
  
  #% If the single found point is inside the envelope, the found intercept
  #% is at the end. Therefore extend the start
  if inpolygon(charAvg[indexIntercept[1],0], charAvg[indexIntercept[1],0], envelope[:,0],envelope[:,1]): 
    iIntEnd = indexIntercept[1]
    linestart_1 = interpolate.interp1d(np.concatenate([[0],aLenInterval]),charAvg[0:1,0], kind='linear',fill_value="extrapolate")(-aLenExtension)
    linestart_2 = interpolate.interp1d(np.concatenate([[0],aLenInterval]),charAvg[0:1,1], kind='linear',fill_value="extrapolate")(-aLenExtension)

    lineStart =  np.vstack(np.concatenate([linestart_1 , linestart_2]), charAvg[0:indexLength,:])

  #%Find intercepts to divide line using Poly
    _, _, iIntStart = polyxpoly(closedEnvelope[:,0],closedEnvelope[:,1],lineStart[:,0],lineStart[:,1])
    iIntStart = iIntStart[0]

  #% If the single found point is outside the envelope, the found
  #% intercept is the start
  else:
    iIntStart = indexIntercept[0]
    lineend_1 = interpolate.interp1d(np.array([0,1-aLenInterval]), np.concatenate([charAvg[-1,0],charAvg[-1-1,0]]), kind='linear',fill_value="extrapolate")(1+aLenExtension)
    lineend_2 = interpolate.interp1d(np.array([0,1-aLenInterval]), np.concatenate([charAvg[-1,1],charAvg[-1-1,1]]), kind='linear',fill_value="extrapolate")(1+aLenExtension)

    lineEnd =  np.vstack(charAvg[-1-indexLength:-1,:], np.concatenate([lineend_1 , lineend_2]))
    
    #%Find intercepts to divide line using Poly
    _, _, iIntEnd = polyxpoly(closedEnvelope[-1,0],closedEnvelope[:,1], lineEnd[:,0],lineEnd[:,1])
    iIntEnd = iIntEnd[0]
    
#% If we find no intercepts, we need to extend both sides of characteristic
#% average to intercept the envelop.
else:
  aLenInterval = 1/nResamplePoints
  indexLength = round(0.2*len(charAvg))
  
  aLenExtension = np.abs(aLenInterval/(charAvg[0,:]-charAvg[1,:])) * 1.1 * np.max(stdevData)
  aLenExtension[aLenExtension == inf] = 0
  aLenExtension = max(aLenExtension)

  linestart_1 = interpolate.interp1d(np.concatenate([[0],aLenInterval]),charAvg[0:1,0], kind='linear',fill_value="extrapolate")(-aLenExtension)
  linestart_2 = interpolate.interp1d(np.concatenate([[0],aLenInterval]),charAvg[0:1,1], kind='linear',fill_value="extrapolate")(-aLenExtension)
  lineStart =  np.vstack(np.concatenate([linestart_1 , linestart_2]), charAvg[0:indexLength,:])
  
  lineend_1 = interpolate.interp1d([0,1-aLenInterval], np.concatenate([charAvg[-1,0],charAvg[-1-1,0]]), kind='linear',fill_value="extrapolate")(1+aLenExtension)
  lineend_2 = interpolate.interp1d([0,1-aLenInterval], np.concatenate([charAvg[-1,1],charAvg[-1-1,1]]), kind='linear',fill_value="extrapolate")(1+aLenExtension)
  lineEnd =  np.vstack(charAvg[-1-indexLength:-1,:], np.concatenate([lineend_1 , lineend_2]))
      
  #%Find intercepts to divide line using Poly
  _, _, iIntStart = polyxpoly(closedEnvelope[:,0],closedEnvelope[:,1], lineStart[:,0],lineStart[:,1])
  iIntStart = iIntStart[0]
    
  _, _, iIntEnd = polyxpoly(closedEnvelope[:,0],closedEnvelope[:,1], lineEnd[:,0],lineEnd[:,1])
  iIntEnd = iIntEnd[0]

#% To divide inner or outer corridors, first determine if polygon is clockwise
#% or counter-clockwise. Then, based on which index is large, separate out
#% inner and outer corridor based on which intercept index is larger. 

if ispolycw(envelope[:,0],envelope[:,1]):
  if iIntStart > iIntEnd:
    outerCorr = np.vstack([envelope[iIntStart:-1,:],envelope[0:iIntEnd,:]])
    innerCorr = envelope[iIntEnd:iIntStart,:]
  else:
    outerCorr = envelope[iIntStart:iIntEnd,:]
    innerCorr = np.vstack([envelope[iIntEnd:-1,:], envelope[0:iIntStart,:]])
else:
  if iIntStart > iIntEnd:
    innerCorr = np.vstack([envelope[iIntStart:-1,:], envelope[0:iIntEnd,:]])
    outerCorr = envelope[iIntEnd:iIntStart,:]
  else:
    innerCorr = envelope[iIntStart:iIntEnd,:]
    outerCorr = np.vstack([envelope[iIntEnd:-1,:],envelope[0:iIntStart,:]])


#% Resample corridors. Use nResamplePoints. Because corridors are
#% non-monotonic, arc-length method discussed above is used. 
#% Start with inner corridor. Magnitudes are being normalized.
segments = np.sqrt(((innerCorr[0:-1,0]-innerCorr[1:,0]) / np.max(innerCorr[:,0])) **2 + ((innerCorr[0:-1,1]-innerCorr[1:,1])/np.max(innerCorr[:,1])) **2)
alen = np.cumsum(np.concatenate([[0],segments]))
alenResamp = np.linspace(0,np.max(alen), num = nResamplePoints)
alenResamp = np.transpose(alenResamp)
innerCorr = np.column_stack([interpolate.interp1d(alen,innerCorr[:,0])(alenResamp), interpolate.interp1d(alen,innerCorr[:,1])(alenResamp)])

#% Outer Corridor
segments = np.sqrt(((outerCorr[0:-1,0]-outerCorr[1:,0]) / np.max(outerCorr[:,0])) **2 + ((outerCorr[0:-1,1]-outerCorr[1:,1])/np.max(outerCorr[:,1])) **2)
alen = np.cumsum(np.concatenate([[0],segments]))
alenResamp = np.linspace(0,np.max(alen), num = nResamplePoints)
alenResamp = np.transpose(alenResamp)
outerCorr = np.column_stack([interpolate.interp1d(alen,outerCorr[:,0])(alenResamp), interpolate.interp1d(alen,outerCorr[:,1])(alenResamp)])


#%% Draw extension lines and sampling points to MS plot
if Diagnostics == 'on':
  #% Plot corridors, avgs
  plt.scatter(xx[:],yy[:],zz[:]>=1)
  
  plt.xlabel('x - axis')
  # Set the y axis label of the current axis.
  plt.ylabel('y - axis')
# Set a title of the current axes.
  plt.suptitle('Ellipses and Corridor Extraction ')
# show a legend on the plot
# Display a figure.
  plt.show()

if NormalizeSignals == 'on':
  for iSignal in inputSignals.keys():
    #% Resulting array is normalized arc - length, resampled x, resam.y
    plt.plot(normalizedSignal[iSignal][:,1], normalizedSignal[iSignal][:,2], label = 'Input Signals', c ='grey')
  plt.title('Normalization')
else:
  for iSignal in inputSignals.keys():

    plt.plot(inputSignals[iSignal][:,0], inputSignals[iSignal][:,1], label = 'Input Signals', c ='grey')
  plt.title('No Normalization')

plt.plot(charAvg[:,0], charAvg[:,1], label = 'Average - ARCGen', c ='black')
plt.scatter(outerCorr[:,0], outerCorr[:,1], label = 'Corridors', c ='yellow')
plt.plot(innerCorr[:,0], innerCorr[:,1])

plt.xlabel('x - axis')
# Set the y axis label of the current axis.
plt.ylabel('y - axis')
# Set a title of the current axes.
# show a legend on the plot
plt.legend()
# Display a figure.
plt.show()